{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood  - Classification Using Logistic Regression\n",
    "\n",
    "<h3> \n",
    "Aaron Trefler <br/>\n",
    "JPL <br/>\n",
    "Created: 07/11/2016 <br/>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "\n",
    "from flood_functions import grace_brick_convert_lowres \n",
    "from flood_functions import calculate_confusion_matrix_bricks\n",
    "from IPython.display import display, HTML\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define directories\n",
    "dir_flood = '../../Raw Data/Flood Observatory/'\n",
    "dir_python_data = '../Data/'\n",
    "dir_grace = '../../Work_Matlab/Data/'\n",
    "dir_figures = '../Figures/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRACE MASCON-GRI Pickle Files\n",
    "f = open(dir_python_data+'grace_features.p', 'rb')\n",
    "grace_features_dict = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assign grace maps to python objects\n",
    "\n",
    "# continuous\n",
    "grace_lwe = grace_features_dict['grace_lwe']\n",
    "grace_lwe_norm = grace_features_dict['grace_lwe_norm']\n",
    "grace_lwe_clim_norm = grace_features_dict['grace_lwe_clim_norm']\n",
    "grace_lwe_noClim_norm = grace_features_dict['grace_lwe_noClim_norm']\n",
    "\n",
    "# ranked\n",
    "grace_lwe_rank = grace_features_dict['grace_lwe_rank']\n",
    "grace_lwe_rank_norm = grace_features_dict['grace_lwe_rank_norm']\n",
    "grace_lwe_clim_rank = grace_features_dict['grace_lwe_clim_rank']\n",
    "grace_lwe_clim_rank_norm = grace_features_dict['grace_lwe_clim_rank_norm']\n",
    "grace_lwe_noClim_rank = grace_features_dict['grace_lwe_noClim_rank']\n",
    "grace_lwe_noClim_rank_norm = grace_features_dict['grace_lwe_noClim_rank_norm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flood Observatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flood Event Brick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(dir_python_data+'flood_event_brick.p', 'rb')\n",
    "flood_event_brick = pickle.load(f)\n",
    "f.close()\n",
    "flood_event_brick_highres = flood_event_brick[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# name of preprocessed and merged flood dataframe\n",
    "df_name = 'df_flood_grace_time_location_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FO data\n",
    "df_flood_grace = pd.read_csv(dir_python_data + df_name + '.csv')\n",
    "df_flood_grace = df_flood_grace.drop('Unnamed: 0', axis=1)\n",
    "#df_flood_grace.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRACE\n",
    "dim = grace_lwe.shape\n",
    "d1 = dim[0]\n",
    "d2 = dim[1]\n",
    "tp = dim[2]\n",
    "# FO\n",
    "floods = len(df_flood_grace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define feature brick (NaN's for non-land regions)\n",
    "feature_brick_highres = grace_lwe_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create low resolution feature brick (0's and 1's)\n",
    "feature_brick = grace_brick_convert_lowres(feature_brick_highres, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create low resolution flood brick (0's and 1's)\n",
    "flood_event_brick_lowres = grace_brick_convert_lowres(flood_event_brick_highres, 6)\n",
    "flood_event_brick_lowres[flood_event_brick_lowres > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create binary flood zone map and brick\n",
    "flood_zone = np.nansum(flood_event_brick_lowres,axis=2)\n",
    "flood_zone[flood_zone > 0] = 1\n",
    "flood_zone = flood_zone[:,:,np.newaxis]\n",
    "flood_zone_brick = np.tile(flood_zone,(1,1,152))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create label brick (NaNs for non-flood regions)\n",
    "label_brick = flood_event_brick_lowres\n",
    "label_brick[flood_zone_brick==0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create prediction brick\n",
    "pred_brick = np.empty([d1_lowres,d2_lowres,tp])\n",
    "pred_brick.fill(np.nan)\n",
    "\n",
    "# perform logistic regression\n",
    "for i in range(d1_lowres):\n",
    "    \n",
    "    for j in range(d2_lowres):\n",
    "        \n",
    "        features = feature_brick[i,j,:]\n",
    "        labels = label_brick[i,j,:].astype(int)\n",
    "        \n",
    "        # add column of 1's to features\n",
    "        ones = np.ones(len(features))\n",
    "        features = np.vstack((ones,features)).transpose()\n",
    "              \n",
    "        if (np.nansum(labels) > 0): #flood events occured\n",
    "                \n",
    "            clf = LogisticRegression(fit_intercept=False)\n",
    "            clf.fit(features, labels)\n",
    "            pred = clf.predict(features)\n",
    "            pred_brick[i,j,:] = pred    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create prediction brick\n",
    "pred_brick = np.empty([d1_lowres,d2_lowres,tp])\n",
    "pred_brick.fill(np.nan)\n",
    "\n",
    "# perform threshold classification\n",
    "for i in range(d1_lowres):\n",
    "    \n",
    "    for j in range(d2_lowres):\n",
    "        \n",
    "        features = feature_brick[i,j,:]\n",
    "        labels = label_brick[i,j,:].astype(int)\n",
    "              \n",
    "        if (np.nansum(labels) > 0): #flood events occured\n",
    "                \n",
    "            pred = features >= 100\n",
    "            pred_brick[i,j,:] = pred    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Hypothesis Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create null hypothesis prediction brick\n",
    "pred_null_brick = np.empty([d1_lowres,d2_lowres,tp])\n",
    "pred_null_brick.fill(np.nan)\n",
    "\n",
    "# fill in null hypothesis birck with all 0's in areas with flood events\n",
    "for i in range(d1_lowres):\n",
    "    \n",
    "    for j in range(d2_lowres):\n",
    "        \n",
    "        labels = label_brick[i,j,:].astype(int)\n",
    "              \n",
    "        if (np.nansum(labels) > 0): #flood events occured\n",
    "                \n",
    "            pred_null_brick[i,j,:] = 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predcitions\n",
    "pred_confusion_matrix = calculate_confusion_matrix_bricks(label_brick, pred_brick)\n",
    "pred_tp_brick = pred_confusion_matrix['true_positive'] #hit\n",
    "pred_fp_brick = pred_confusion_matrix['true_negative'] #type I error\n",
    "pred_fn_brick = pred_confusion_matrix['false_positive'] #type II error\n",
    "pred_tn_brick = pred_confusion_matrix['false_negative'] #correct regection\n",
    "\n",
    "# null hypothesis\n",
    "pred_null_confusion_matrix = calculate_confusion_matrix_bricks(label_brick, pred_null_brick)\n",
    "pred_null_tp_brick = pred_null_confusion_matrix['true_positive'] #hit\n",
    "pred_null_fp_brick = pred_null_confusion_matrix['true_negative'] #type I error\n",
    "pred_null_fn_brick = pred_null_confusion_matrix['false_positive'] #type II error\n",
    "pred_null_tn_brick = pred_null_confusion_matrix['false_negative'] #correct regection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# create confusion matrix bricks\n",
    "metric_tp_brick = np.zeros([d1_lowres,d2_lowres,tp]) #hit\n",
    "metric_fp_brick = np.zeros([d1_lowres,d2_lowres,tp]) #type I error\n",
    "metric_fn_brick = np.zeros([d1_lowres,d2_lowres,tp]) #type II error\n",
    "metric_tn_brick = np.zeros([d1_lowres,d2_lowres,tp]) #correct regection\n",
    "\n",
    "for i in range(d1_lowres):\n",
    "    for j in range(d2_lowres):\n",
    "        for k in range(tp):\n",
    "            \n",
    "            if(pred_brick[i,j,k] == 1 and label_brick[i,j,k] == 1):\n",
    "                metric_tp_brick[i,j,k] = 1\n",
    "            elif(pred_brick[i,j,k] == 1 and label_brick[i,j,k] == 0):\n",
    "                metric_fp_brick[i,j,k] = 1\n",
    "            elif(pred_brick[i,j,k] == 0 and label_brick[i,j,k] == 0):\n",
    "                metric_tn_brick[i,j,k] = 1\n",
    "            elif(pred_brick[i,j,k] == 0 and label_brick[i,j,k] == 1):\n",
    "                metric_fn_brick[i,j,k] = 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create ML metric maps\n",
    "\n",
    "#accuracy\n",
    "metric_accuracy_map = np.empty([d1_lowres,d2_lowres]) \n",
    "metric_accuracy_map.fill(np.nan)\n",
    "#miss-classifcation rate\n",
    "metric_missClassRate_map = np.empty([d1_lowres,d2_lowres]) \n",
    "metric_missClassRate_map.fill(np.nan)\n",
    "#true positive rate\n",
    "metric_tpRate_map = np.empty([d1_lowres,d2_lowres]) \n",
    "metric_tpRate_map.fill(np.nan)\n",
    "#false positive rate\n",
    "metric_fpRate_map = np.empty([d1_lowres,d2_lowres]) \n",
    "metric_fpRate_map.fill(np.nan)\n",
    "#specificity\n",
    "metric_specificity_map = np.empty([d1_lowres,d2_lowres]) \n",
    "metric_specificity_map.fill(np.nan)\n",
    "#precision\n",
    "metric_precision_map = np.empty([d1_lowres,d2_lowres]) \n",
    "metric_precision_map.fill(np.nan)\n",
    "#prevalence\n",
    "metric_prevalence_map = np.empty([d1_lowres,d2_lowres]) \n",
    "metric_prevalence_map.fill(np.nan)\n",
    "\n",
    "for i in range(d1_lowres):\n",
    "    for j in range(d2_lowres):\n",
    "    \n",
    "        if (np.nansum(label_brick[i,j,:]) > 0): #flood events occured\n",
    "            \n",
    "            metric_accuracy_map[i,j] = \\\n",
    "                (np.sum(metric_tp_brick[i,j,:]) + np.sum(metric_tn_brick[i,j,:])) / tp\n",
    "            metric_missClassRate_map[i,j] = \\\n",
    "                (np.sum(metric_fp_brick[i,j,:]) + np.sum(metric_fn_brick[i,j,:])) / tp\n",
    "            metric_tpRate_map[i,j] = \\\n",
    "                np.sum(metric_tp_brick[i,j,:]) /  np.sum(label_brick[i,j,:])\n",
    "            metric_fpRate_map[i,j] = \\\n",
    "                np.sum(metric_fp_brick[i,j,:]) /  np.sum(label_brick[i,j,:] == 0)\n",
    "            metric_specificity_map[i,j] = \\\n",
    "                np.sum(metric_tn_brick[i,j,:]) /  np.sum(label_brick[i,j,:] == 0)\n",
    "            metric_precision_map[i,j] = \\\n",
    "                np.sum(metric_tp_brick[i,j,:]) /  np.sum(pred_brick[i,j,:] == 1)\n",
    "            metric_prevalence_map[i,j] = \\\n",
    "                np.sum(label_brick[i,j,:] == 1)  / tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(3,3,1)\n",
    "plt.imshow(np.flipud(metric_accuracy_map.transpose()), vmin=0, vmax=1)\n",
    "plt.colorbar(orientation='horizontal')\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.subplot(3,3,2)\n",
    "plt.imshow(np.flipud(metric_missClassRate_map.transpose()), vmin=0, vmax=1)\n",
    "plt.colorbar(orientation='horizontal')\n",
    "plt.title('Error Rate')\n",
    "\n",
    "plt.subplot(3,3,3)\n",
    "plt.imshow(np.flipud(metric_tpRate_map.transpose()), vmin=0, vmax=1)\n",
    "plt.colorbar(orientation='horizontal')\n",
    "plt.title('True Positive Rate')\n",
    "\n",
    "plt.subplot(3,3,4)\n",
    "plt.imshow(np.flipud(metric_fpRate_map.transpose()), vmin=0, vmax=1)\n",
    "plt.colorbar(orientation='horizontal')\n",
    "plt.title('False Positive Rate')\n",
    "\n",
    "plt.subplot(3,3,5)\n",
    "plt.imshow(np.flipud(metric_specificity_map.transpose()), vmin=0, vmax=1)\n",
    "plt.colorbar(orientation='horizontal')\n",
    "plt.title('Specificity')\n",
    "\n",
    "plt.subplot(3,3,6)\n",
    "plt.imshow(np.flipud(metric_precision_map.transpose()), vmin=0, vmax=1)\n",
    "plt.colorbar(orientation='horizontal')\n",
    "plt.title('Precision')\n",
    "\n",
    "plt.subplot(3,3,7)\n",
    "plt.imshow(np.flipud(metric_prevalence_map.transpose()), vmin=0, vmax=1)\n",
    "plt.colorbar(orientation='horizontal')\n",
    "plt.title('Prevalence')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
